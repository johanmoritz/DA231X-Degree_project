\chapter{Method}
\label{ch:methods}
% \todo[inline, backgroundcolor=kth-lightblue]{Metod eller Metodval}
% \todo[inline]{This chapter is about Engineering-related
% 	content, Methodologies and Methods.  Use a self-explaining title.\\The
% 	contents and structure of this chapter will change with your choice of
% 	methodology and methods.}


% Thoughts on methodology:
% - Idea: Create asset management of verified \texttt{.buildinfo} files through smart contracts on the Hyperledger Fabric blockchain
%
%	Method:
%		1. Model relevant parts of known/working blockchain together with chaincode in TLA+
% 		2. Implement chaincode for reproducible builds in TLA+
%		3. Verify safety and liveness properties of the system with reproducible builds chaincode with TLC
%		4. Translate reproducible builds chaincode model (from step 2) into actual chaincode (in Java/Go/Typescript)
%		5. Evaluate functional requirements of the actual system with the reprodcible builds chaincode (from step 4)
%
%	Evaluation of system model:
%		A.
% 			a) Model the system in step 1. from system logs
%			b) Evaulate model based on informal transaction flow description
%
%		B.
% 			a) Model the system in step 1. from informal transaction flow
%			b) Evaluate model based on system logs
%
%		C.
% 			a) Model the system in step 1. from both system logs and informal transaction flow
%			b) No system model evaluation. Assume correct and evaluate through method steps 3 and 5
%
%	Additional steps
% 		- Which evaluation strategy (A, B or C) should we use?
%		- Decide/Motivate what parts of the initial system that needs to be modelled.
% 			What assumptions are made on the rest of the (non-modelled) system?
% 		- Decide what functional requirements the finished system should conform to? How do we test them? User stories?
% 		- Decide what safety and liveness properties we should test on the model?
% 		- Decide how to model the chaincode API
% 		- Decide which endorsement policies are needed i.e. who is allowed to change a record. How is this represented in the model?
% 		- Figure out if world state is updated while simulating a chaincode. I.e. does PutState update the worldstate at once, or does it change. Partial answer: The version of a key is the blockchain height of the peer that commited the latest transaction that changed the particular key.
%
% 	Model Assumptions:
% 		- All and only the chaincodes that we create are available on the blockchain
% 		- The ledger itself - we only use the key-value state of the world
% 		- ==> Versioning of worldstate is done incrementally
% 		- All nodes are authenticated by their respective organizations CA
%
%
% 	Purpose of doing formal specification:
% 		- Testing the behaviour of the composition of chaincodes over time
% 		- Testing the behaviour of "actors" when running the chaincode
%
% 	Some "bad" behaviours:
% 		- Peers add different \texttt{.buildinfo} files all the time and none is verified by multiple peers
% 		- Peers don't actually build packages, instead they just pretend that they did
%
%
% 	"Dutch auction" asset management solution:
% 		1. For each new \texttt{.buildinfo} file: a submission process that looks like an auction begins. A unique_auction_id is created and stored on the ledger.
% 			- Not all organizations can start a submission process: only the once that behave well in other auctions (what does this mean?)
% 		2. Each builder creates a submission S = Hash(build + unique_auction_id) and PublicS = Hash(S + unique_submitter_id) and runs a transaction which stores PublicS on the ledger and S in a private collection
% 		3. On a certain condition (what?): each builder validates all PrivateS submissions (because they have S stored in a private collection) and a consensus of whether or not each submission is valid is reached. The voting scores (valid or invalid) for the buildinfo file is tallied and stored on the ledger. Each builder who did their job correctly (voted with the majority?) gets a reward.
%
% 			Question: What happens with invalid i.e. non-reproducible builds? How are they represented as a submission?
% 			Question: How is the reward formulated/connection to who can start a submission process?
% 			Thought: Malicious organization vs malicious node
%
%
%
%
% =======================================================
% Majority decision judgment with reward feedback loop
% -------------------------------------------------------
%
% Definitions:
% 	- Judgment: whether a .buildinfo file is valid or not
% 	- Package preferences: packages that a particular builder wants to have valid judgments for
% 	- BuildToken: internal currency, shortform BT
%
% Assumptions:
% 	- There are byzantine and non-byzantine builders
% 	- All non-byzantine builders want valid judgments for some packages (!= {}) (that they rely on themselves)
%
% 	- Non-byzantine builders prefer to judge some packages (their package preferences) over other
% 	- For all packages p, there exists at least one non-byzantine builder who prefer to build p
% 	- Byzantine builders may collude with each other, non-byzantine builders may not (their package preferences are private)
% 	- (Each organization has one peer/builder)
%
% Safety: Only valid judgements are stored
% Liveness: Every .buildinfo file is eventually judged (builders cannot starve only some judgements)
%
% Judgement algorithm:
% 	Given the name f_name and checksum f_hash of a .buildinfo file,
% 			  builders b1,b2, ..., bk,
% 			  rebuilding function B,
% 			  cryptographic hash function H,
% 			  HMAC function HMAC, and
%		 	  world state W1 = W2 = W3 = W4 = {},
% 	judge the validity and reproducibility of f_name.
% 	 1. For each builder bi:
% 		a) Pick key ki
% 		b) Build the package p = B(f_name)
% 		c) Calculate p:s checksum h = H(p)
% 		c) Verify that f_name should create p (f_name=>p)
% 		d) ei = if f_name=>p then HMAC(ki, h) else HMAC(ki, "invalid")
% 		e) Store ki, h privately
% 		f) W1[bi] = ei
% 	2. When appropriate (timeout or |W1|): lock W1 from changing
% 	3. For each builder bi: W2[bi] = ei
% 	4. When appropriate (timeout or |W2|): lock W2 from changing
% 	5. For each builder bi: calculate the validity of builder bi:s work:
% 			W3[bi] = W1[bi] == HMAC(W2[bi], h) || W1[bi] == HMAC(W2[bi], "invalid")
% 	6. For each builder bi: judge f_name
% 		if W3[bi]
% 			then W4 = if W1[bi] == HMAC(W2[bi], h)
% 						then True  // valid .buildinfo
% 						else False // invalid .buildinfo
% 	7. For each builder bi where W3[bi] = True: BT[bi] += reward
%
% Reward feedback loop algorithm:
% 	Alt1:
% 		- Get 1 BT for judging correctly
% 		- Pay 1 BT for requesting a judgement
%
% 	Alt2:
% 		- Anyone can judge or request a judgment (no cost or reward)
%
% 	Alt3:
% 		- Auction to decide cost and reward
%
% 	Alt4:
% 		- Cost and reward relative to work input (e.g. buildtime)
%
% "Vulnerabilities":
% 	- Builder can take a valid build from somewhere else and claim that they built it
% 	- Builder can upload an invalid judgment
% 	- Builder can choose not to judge a package
%



% \todo[inline]{Describe the engineering-related contents (preferably with models) and the research methodology and methods that are used in the degree project.\\
% 	Give a theoretical description of the scientific or engineering methodology are you going to use and why have you chosen this method. What other methods did you consider and why did you reject them.\\
% 	In this chapter, you describe what engineering-related and scientific skills you are going to apply, such as modeling, analyzing, developing, and evaluating engineering-related and scientific content. The choice of these methods should be appropriate for the problem . Additionally, you should be consciousness of aspects relating to society and ethics (if applicable). The choices should also reflect your goals and what you (or someone else) should be able to do as a result of your solution - which could not be done well before you started.}

% % The purpose of this chapter is to provide an overview of the research method
% % used in this thesis. Section~\ref{sec:researchProcess} describes the research
% % process. Section~\ref{sec:researchParadigm} details the research
% % paradigm. Section~\ref{sec:dataCollection} focuses on the data collection
% % techniques used for this research. Section~\ref{sec:experimentalDesign}
% % describes the experimental design. Section~\ref{sec:assessingReliability}
% % explains the techniques used to evaluate the reliability and validity of the
% % data collected. Section~\ref{sec:plannedDataAnalysis} describes the method
% % used for the data analysis. Finally, Section~\ref{sec:evaluationFramework}
% % describes the framework selected to evaluate xxx.

% \todo[inline, backgroundcolor=kth-lightblue]{Vilka vetenskapliga eller ingenjörsmetodik ska du använda och varf��r har du valt den här metoden. Vilka andra metoder gjorde du överväga och varför du avvisar dem.
% 	Vad är dina mål? (Vad ska du kunna göra som ett resultat av din lösning - vilken inte kan göras i god tid innan du började)
% 	Vad du ska göra? Hur? Varför? Till exempel, om du har implementerat en artefakt vad gjorde du och varför? Hur kommer ditt utvärdera den.
% 	Syftet med detta kapitel är att ge en översikt över forsknings metod som
% 	används i denna avhandling. Avsnitt~\ref{sec:researchProcess} beskriver forskningsprocessen. Avsnitt~\ref{sec:researchParadigm} detaljer forskningen paradigm. Avsnitt~\ref{sec:dataCollection} fokuserar på datainsamling
% 	tekniker som används för denna forskning. Avsnitt~\ref{sec:experimentalDesign} beskriver experimentell
% 	design. Avsnitt~\ref{sec:assessingReliability} förklarar de tekniker som används för att utvärdera
% 	tillförlitligheten och giltigheten av de insamlade uppgifterna. Avsnitt~\ref{sec:plannedDataAnalysis}
% 	beskriver den metod som används för dataanalysen. Slutligen, Avsnitt~\ref{sec:evaluationFramework}
% 	beskriver ramverket valts för att utvärdera xxx.\\
% 	Ofta kan man koppla ett antal följdfrågor till undersökningsfrågan och problemlösningen t ex\\
% 	(1) Vilken process skall användas för konstruktion av lösningen och vilken process skall kopplas till denna för att svara på undersökningsfrågan?\\
% 	(2) Hur och vilket resultat (storheter) skall presenteras både för att redovisa svar på undersökningsfrågan (resultatkapitlet i denna rapport) och redovisa resultat av problemlösningen (prototypen, ofta dokument som bilagor men vilka dokument och varför?).\\
% 	(3) Vilken teori/teknik skall väljas och användas både för undersökningen (taxonomi, matematik, grafer, storheter mm)  och  problemlösning (UML, UseCases, Java mm) och varför?\\
% 	(4) Vad behöver du som student leverera för att uppnå hög kvaliet (minimikrav) eller mycket hög kvalitet på examensarbetet?\\
% 	(5) Frågorna kopplar till de följande underkapitlen.\\
% 	(6) Resonemanget bygger på att studenter på hing-programmet ofta skall konstruera något åt problemägaren och att man till detta måste koppla en intressant ingenjörsfråga. Det finns hela tiden en dualism mellan dessa aspekter i exjobbet.
% }

\section{Research Process}
\label{sec:researchProcess}
% \todo[inline, backgroundcolor=kth-lightblue]{Undersökningsrocess och utvecklingsprocess}

% % Figure~\ref{fig:researchprocess} shows the steps conducted in order to carry out this research.

% \todo[inline, backgroundcolor=kth-lightblue]{Figur~\ref{fig:researchprocess} visar de steg som utförs för att genomföra\\
% 	Beskriv, gärna med ett aktivitetsdiagram (UML?), din undersökningsprocess och utvecklingsprocess.  Du måste koppla ihop det akademiska intresset (undersökningsprocess) med ursprungsproblemet (utvecklingsprocess)
% 	denna forskning.\\
% 	Aktivitetsdiagram från t ex UML-standard}



% % \begin{figure}[!ht]
% % 	\begin{center}
% % 		\includegraphics[width=0.5\textwidth]{figures/researchprocess.png}
% % 	\end{center}
% % 	\caption{Research Process}
% % 	\label{fig:researchprocess}
% % \end{figure}
% % \todo[inline, backgroundcolor=kth-lightblue]{Forskningsprocessen}

One of the primary goals of the research process is to construct and evaluate a system for enhanced traceability and integrity of \texttt{.buildinfo} files. The main steps of this process include (I) defining the participating actors of such a system, what is assumed from them and what it means for the system to be correct, (II) selecting a \glsentrylong{DLT} on which to build the system, (III) designing appropriate algorithms taking the underlying technology and assumptions under consideration, (IV) modeling the system in a formal specification tool, (V) evaluating the system correctness within the model, and (VI) evaluate the feasability to construct an implementation of the system. Steps (III) to (VI) are in practice done iteratively to raise the reliability and validity of the result.

% \section{Research Paradigm}
% \label{sec:researchParadigm}
% \todo[inline, backgroundcolor=kth-lightblue]{Undersökningsparadigm\\
% 	Exempelvis\\
% 	Positivistisk (vad/hur fungerar det?) kvalitativ fallstudie med en deduktivt (förbestämd) vald ansats och ett induktivt(efterhand uppstår dataområden och data) insamlade av data och erfarenheter.}


% \section{Data Collection}
% \label{sec:dataCollection}
% \todo[inline]{This should also show that you are aware of the social and ethical concerns that might be relevant to your data collection method.)}

% \todo[inline, backgroundcolor=kth-lightblue]{Datainsamling\\
% 	(Detta bör också visa att du är medveten om de sociala och etiska frågor som
% 	kan vara relevanta för dina data insamlingsmetod.)}

% \subsection{Sampling}
% \todo[inline, backgroundcolor=kth-lightblue]{Stickprovsundersökning}

% \subsection{Sample Size}
% \todo[inline, backgroundcolor=kth-lightblue]{Provstorleken}

% \subsection{Target Population}
% \todo[inline, backgroundcolor=kth-lightblue]{Målgruppen}

% \section{Experimental design/Planned Measurements}
% \label{sec:experimentalDesign}
% \todo[inline, backgroundcolor=kth-lightblue]{Experimentdesign/Mätuppställning}

% \subsection{Test environment/test bed/model}\todo[inline]{Describe everything that someone else would need to reproduce your test environment/test bed/model/… .}
% \todo[inline, backgroundcolor=kth-lightblue]{Testmiljö/testbädd/modell\\
% 	Beskriv allt att någon annan skulle behöva återskapa din testmiljö / testbädd / modell / …}

% \subsection{Hardware/Software to be used}
% \todo[inline, backgroundcolor=kth-lightblue]{Hårdvara / programvara som ska användas}


% \section{Assessing reliability and validity of the data collected}
% \label{sec:assessingReliability}
% \todo[inline, backgroundcolor=kth-lightblue]{Bedömning av validitet och reliabilitet hos använda metoder och insamlade data }


\subsection{Validity of method}
\label{sec:validtyOfMethod}
% \todo[inline]{How will you know if your results are valid?}
% \todo[inline, backgroundcolor=kth-lightblue]{Giltigheten av metoder\\
% 	Har dina metoder ge dig de rätta svaren och lösning? Var metoderna korrekt?}

Throughout the methodology, an assumption is made that the formal specification tool TLA\textsuperscript+ can be used to ensure the validity of the system model with regards to underlying assumptions and correctness specifications. This is a slightly naive perspective as the model checking is bounded in size and not a formal proof, but is intuitively reasonable because the studied behaviours (the interactions with the system) are themselves bounded by time.

Given that the modeling tools have these properties, the validity of the method is based on arguing the validity of the system assumptions and that an implementation of the system is feasable given the model. The first of these arguments is done by specifying assumptions and correctness definitions in a simple and succinct manner and connect them closely to the context and purpose within which they act. This way, the validity of the system assumptions should be intuitive to the reader. The second argument revolves around building a prototype implementation of relevant parts of the system and motivate the translation from model to prototype.

\subsection{Reliability of method}
\label{sec:reliabilityOfMethod}
% \todo[inline]{How will you know if your results are reliable?}
% \todo[inline, backgroundcolor=kth-lightblue]{Tillförlitlighet av metoder\\
% 	Hur bra är dina metoder, finns det bättre metoder? Hur kan du förbättra dem?}

The use of a system model written in TLA\textsuperscript+ instead of an actual implementation is only as reliable as they are similar to each other. If the model is unfaithful in its representation of the system, then very little can be reliably stated based on the method. However, assuming the model corresponds well with the implementation, it is a very exact tool. The crucial part is to define the model boundaries so as to not confuse the reader under which conditions the method is reliable.

The ideal method for this project would be building the system and evaluating it with real world actors interessted in using it. That would give a clear indication of the systems' properties and the outcomes from utilizing it. It would, however, be quite out of scope for this project. To diminish the distance between this ideal method and the one used, the model is made in such a way that the number of assumptions on actors are as few as possible and clearly formulated. As every assumption is encoded in TLA\textsuperscript+, they are defined semantically and can be verified by the reader.

\section{Model evaluation}
\label{sec:model_evaluation}

To evaluate the correctness of the TLA\textsuperscript+ model, a number of scenarios are tested with the TLC model checker. Because TLC is a bounded model checker, these scenarios do not prove the correctness of the model in general but gives an indication to under which assumptions it is correct. Particularly, they are selected to show that the system makes progress in non-trivial cases, rather than stopping in a deadlocked state, and that it yields valid results. In all scenarios, a limit as well as goal is set in terms of the number of packages the system should evaluate correctly before finishing. In other words, for each behaviour (or state sequence) that is tested in a scenario, either the goal is reached or the model has failed the scenario.

\subsection{Scenario 1: No malicious builders}
\label{subsec:scenario_non-malicious}

This scenario is a positive test of the model where every builder is assumed to take egoistical but non-malicious actions, following the behaviour described under section \ref{sec:systemActors}. There are three total builders, each one with two preferred packages at a vote target of two. Two of the builders begin with 0BT in their respective wallets while the third one starts with 3BT. This allows the last builder to initialize the first package judgment, because 3BT is exactly the cost for a package with a vote target of two. The judgment goal of the scenario scales from one up to and including four, thus ensuring that the initially preferred packages of at least one builder have been judged.

\subsection{Scenario 2: Single malicious builder}
\label{subsec:scenario_single_malicious}

To test the model under a more realistic scenario, this scenario extends the one in \ref{subsec:scenario_non-malicious} with a malicious builder. The malicious builder acts similarly to the builders from \ref{sec:systemActors}, but will always judge packages incorrectly. With the added malicious builder, this scenario has a total of four builders. To improve model checking time with the additional builder, the judgment goal is lessened to up to and including two packages. The main purpose of this scenario is to ensure that the model works as intended when not all judgments are equal.

\section{Test equipment}

Model checking is run on two different types of hardware. The time-wize shortest tests are run on a laptop with 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz processor and 16.0GB memory. More time-consuming tests are performed on an AWS EC2 c3.8xlarge instance with 32 virtual CPUs and 60GiB of memory. As the performance of the system is not evaluated, the specifics of the test equipment setup is not crucial.

\section{System overview}
\label{sec:systemOverview}

To answer this projects' research question, a system is designed and evaluated for distributing the reproducibility of packages in the \gls{FOSS} ecosystem. The system takes names and checksums of \texttt{.buildinfo} files as input and stores \textit{judgements} on the reproducibility of the packages built from those file. Judgements are created by a consensus of \textit{builders} communicating over a Hyperledger Fabric network, on which they are stored in an append-only blockchain. The packages to be judged are introduced into the system by the builders, after which a voting scheme takes place where the packages are judged as reproducible or not reproducible. Each builders vote represent the result from them, individually, rebuilding the relevant package from its \texttt{.buildinfo} file and verifying wether it was reproducible.

\subsection{System actors}
\label{sec:systemActors}

As described in \ref{sec:background}, the state-of-the-art for verifying reproducible builds are a number of public servers building packages from \texttt{.buildinfo} files and releasing their results separately. The work these \textit{builders} perform would not be changed for the system described in this thesis, only the way they distribute their results. As there is currently no reward besides personal information on package reproducibility for their work, an intial assumption on the motivation of the workers is that

\begin{quote}
	\textit{all builders are interested in valid judgments for some packages.}
	\textbf{(label with a number, like an equation)}
\end{quote}

In other words, every builder has an interest in information regarding the reproducibility of at least one package. These packages are denoted as the builders' \textit{preferences} in this document. Without assuming that every builder has some non-empty prefered set of packages, there is little reason for them to build any packages at all. This is therefore a neccessary requirement of the system. To be noted is that there is nothing stopping a builders set of prefered packages to be exactly all packages. The assumption only states that this set have to be non-empty. The consequence, or definition, of builders having prefered packages is that those are also the packages they prefer to build themselves. This is a natural consequence from the first assumption because a builder would prefer to build a package they are themselves motivated to gain information on. This can be stated as

\begin{quote}
	\textit{builders prefer to judge (and build) some packages over other.}
\end{quote}

This means in particular that any package not prefered by any of the builders would never be built. To counteract this in the model, an additional assumption on the relationship between builders and packages state that

\begin{quote}
	\textit{every package is prefered by at least one builder.}
\end{quote}

While it would be problematic in practice if there are packages that no builder prefers, given that the current public build servers build seemingly every package, this is mainly important for a model definition and not an implementation.

Ensuring that the validity of the judgments on packages made by the system are more trustwordy than those of individual builders, an assumption has to be made that

\begin{quote}
	\textit{builders do not collude or share knowledge with each others through other means than via the system itself.}
\end{quote}

If a builder were to be influenced in their judgment of a package by some other builder than their combined trustwordiness would only be as great as the trustwordiness of the influencer.

\subsection{Model correctness}
\label{sec:modelCorrectness}

The model correctness is defined in terms of safety and liveness properties. As defined in \ref{sec:safetyProperty}, the safety property descibe all the allowed behaviours of the model, while the liveness property ensures that they happen. The properties are stated as


\begin{description}
	\item[Safety] Only valid judgments on \texttt{.buildinfo} files are stored;
	\item[Liveness] Every \texttt{.buildinfo} file is eventually judged, as long as every builder has at least one not-yet judged prefered package.
\end{description}

\textbf{(connect these with the assumptions on builders)}

\textit{Valid judgment} means that if and only if a package is reproducible, it is also judged to be so. In other words, no non-reproducible packages are stored in the system as reproducible and wise versa. As both of these safety and liveness properties are ambiguous by their natural language definitions, they are better stated in semantically defined TLA\textsuperscript+.

Given a set of \texttt{.buildinfo} files $Files$, a mapping from files to \textit{valid} judgments $Validity: Files \mapsto Boolean$, a mapping from files to system judgments $Judgement: Files \mapsto Boolean$ and a mapping with each builders prefered packages $Prefered: Builders \mapsto \{f \in Files'| Files' \subseteq Files\}$, the correctness of the system is defined as

\begin{description}
	\item[Safety] $\forall f \in range(Judgment): Judgment[f] = Validity[f]$
	\item[Liveness] $(\forall b \in Builders: |Prefered[b]| > 0) \implies \Diamond(\forall f \in Files: f \in range(Judgment))$ \textbf{(is this really what you mean?)(Use labels like in an equation)}
\end{description}

% \subsection{Modelling assumptions}
% \label{sec:modellingAssumptions}

% Hyperledger Fabric is selected as the \glsentrylong{DLT} to build the system on.

% The benefits for this project to use Hyperledger Fabric are: permissioned, voting-based, private-collections...

% The modell does not include these features of Hyperledger Fabric: ordering, ...

% instead they are assumed as...

% \subsection{Malicious actors / Threat model}
% \label{sec:maliciousActors}

% Builders control the input to the system.

% We don't care about how builders get their hands on .buildinfo files or other tools.

% Any builder that does not conform to the assumptions in section \ref{sec:systemActors} are denoted as malicious.

% Malicious actors are modelled as: random?, ... \textbf{(how?)}

% \textbf{No malicious actors in model, just discuss how this is solved by the hyperledger fabric.}